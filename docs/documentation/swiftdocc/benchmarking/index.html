<!doctype html><html lang="en-US" class="no-js"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><link rel="icon" href="/swift-docc/favicon.ico"><link rel="mask-icon" href="/swift-docc/favicon.svg" color="#333333"><title>Benchmarking</title><script>var baseUrl = "/swift-docc/"</script><script defer="defer" src="/swift-docc/js/chunk-vendors.0b7dc663.js"></script><script defer="defer" src="/swift-docc/js/index.54db7914.js"></script><link href="/swift-docc/css/index.19043b1e.css" rel="stylesheet"><meta content="Produce performance metrics to improve the compilation pipeline and track regressions." name="description"/></head><body data-color-scheme="auto"><noscript><article><section><ul><li><a href="../index.html">SwiftDocC</a></li><li>Benchmarking</li></ul><p id="eyebrow">API Collection</p><h1>Benchmarking</h1><p id="abstract">Produce performance metrics to improve the compilation pipeline and track regressions.</p></section><h2>Overview</h2><p>DocC has metric logging built right in; it is disabled by default but it can be enabled easily via an environment variable or when running the tools in the <code>bin/benchmark</code> Swift package.</p><p>When you are working on a PR to add a feature or fix a bug you should evaluate the performance cost of your code changes.</p><h2>Running a benchmark</h2><p>To benchmark the <code>convert</code> command with a given documentation catalog <code>MyFramework.docc</code> run:</p><pre><code>swift run --package-path bin/benchmark benchmark --docc-arguments convert MyFramework.docc
</code></pre><p>The tool will enable metrics logging, do five sequential runs of the <code>convert</code> command with the given inputs and options, and write the results to a JSON file on disk.</p><p>For pull requests where you want to compare the local changes against another version of the—the HEAD commits of the branch that the pull request is targeting—you can use the <code>compare-against-commit</code> tool:</p><pre><code>swift run --package-path bin/benchmark benchmark compare-to &lt;commit-ish&gt; --docc-arguments convert MyFramework.docc
</code></pre><p>This tool will gather metrics for both the local changes (same as the default <code>measure</code> tool) and for the other commit of docc, write both results to JSON files on disk, and perform a statistical analysis comparing the two benchmark results.</p><p>If the analysis shows results that you want to investigate further and you want to continue comparing to the same baseline results from the other commit, you can switch to the <code>measure</code> tool and pass the <code>benchmark-&lt;commit-hash&gt;.json</code> file for the <code>--base-benchmark</code> argument.</p><pre><code>swift run --package-path bin/benchmark benchmark measure --base-benchmark benchmark-&lt;commit-hash&gt;.json --docc-arguments convert MyFramework.docc
</code></pre><p>This will only gather new metrics for the local changes, as you iterate, but will still compare the new metrics against the results from the other commit and output the comparison of the two benchmark results.</p><h2>Adding a Custom Metric</h2><p>When you work on a particular feature and you want to track a given custom metric you can temporarily add it to the log.</p><p>For example, to add a metric that counts the registered bundles, create a <code>BundlesCount</code> class that adopts the <a href="../benchmarkmetric/index.html"><code>BenchmarkMetric</code></a> protocol:</p><pre class="swift"><code>class BundlesCount: BenchmarkMetric {
  static let identifier = "bundles-count"
  static let displayName = "Bundles Count"
  
  var result: MetricValue?
  
  init(context: DocumentationContext) {
    result = .number(Double(context.registeredBundles.count))
  }
}
</code></pre><p>Add your custom metric to the default log by using <a href="../benchmark(add:benchmarklog:)/index.html"><code>benchmark(add:benchmarkLog:)</code></a>:</p><pre class="swift"><code>benchmark(add: BundlesCount(context: context))
</code></pre><h2>Topics</h2><h3>Benchmark Log</h3><ul><li><a href="../benchmark/index.html"><code>class Benchmark</code><p>A logger that runs benchmarks and stores the results.</p></a></li><li><a href="../benchmark/main/index.html"><code>static let main: Benchmark</code><p>The shared instance to use for logging.</p></a></li></ul><h3>Default Metrics</h3><ul><li><a href="../benchmark/duration/index.html"><code>class Duration</code><p>A duration metric in milliseconds.</p></a></li><li><a href="../benchmark/datadirectoryoutputsize/index.html"><code>struct DataDirectoryOutputSize</code><p>Measures the output size of the data subdirectory in a DocC archive.</p></a></li><li><a href="../benchmark/indexdirectoryoutputsize/index.html"><code>struct IndexDirectoryOutputSize</code><p>Measures the output size of the index subdirectory in a DocC archive.</p></a></li><li><a href="../benchmark/peakmemory/index.html"><code>class PeakMemory</code><p>A peak memory footprint metric for the current process.</p></a></li><li><a href="../benchmark/topicgraphhash/index.html"><code>class TopicGraphHash</code><p>A hash metric produced off the given documentation context.</p></a></li><li><a href="../benchmark/externaltopicshash/index.html"><code>class ExternalTopicsHash</code><p>A hash metric produced off the externally resolved links.</p></a></li></ul><h3>Logging Metrics</h3><p></p><ul><li><a href="../benchmark(add:benchmarklog:)/index.html"><code>func benchmark&lt;E&gt;(add: @autoclosure () -&gt; E, benchmarkLog: Benchmark)</code><p>Logs a one-off metric value.</p></a></li><li><a href="../benchmark(begin:benchmarklog:)/index.html"><code>func benchmark&lt;E&gt;(begin: @autoclosure () -&gt; E, benchmarkLog: Benchmark) -&gt; E?</code><p>Begins the given metric.</p></a></li><li><a href="../benchmark(end:benchmarklog:)/index.html"><code>func benchmark&lt;E&gt;(end: @autoclosure () -&gt; E?, benchmarkLog: Benchmark)</code><p>Ends the given metric and adds it to the log.</p></a></li><li><a href="../benchmark(wrap:benchmarklog:body:)/index.html"><code>func benchmark&lt;E, Result&gt;(wrap: @autoclosure () -&gt; E, benchmarkLog: Benchmark, body: () throws -&gt; Result) rethrows -&gt; Result</code></a></li></ul></article></noscript><div id="app"></div></body></html>